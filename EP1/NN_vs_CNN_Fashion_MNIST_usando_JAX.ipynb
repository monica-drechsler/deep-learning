{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY3q0Zq8fClr"
      },
      "source": [
        "# NN vs. CNN - Fashion MNIST usando JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFSvUg2YepSD"
      },
      "source": [
        "Referências:\n",
        "\n",
        "NN - https://coderzcolumn.com/tutorials/artificial-intelligence/guide-to-create-simple-neural-networks-using-jax#2\n",
        "\n",
        "CNN - https://coderzcolumn.com/tutorials/artificial-intelligence/jax-guide-to-create-convolutional-neural-networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL2AL4cec-Xz"
      },
      "source": [
        "## Fazendo os imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9dlcu7LFDCm"
      },
      "source": [
        "Import das bibliotecas do JAX para criação das redes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW1wwwO2cmiH",
        "outputId": "e3250d8f-04e4-410c-fba8-8ba818e4335f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX Version : 0.4.26\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "\n",
        "print(\"JAX Version : {}\".format(jax.__version__))\n",
        "\n",
        "from jax.example_libraries import stax, optimizers\n",
        "from jax import numpy as jnp\n",
        "from jax import grad, value_and_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TvpyCjiFNlh"
      },
      "source": [
        "Import de bibliotecas para criar tabelas referentes aos processos de treinamento e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ6jlHiaE__f"
      },
      "outputs": [],
      "source": [
        "import gspread\n",
        "import pytz\n",
        "import time\n",
        "from datetime import datetime\n",
        "from google.colab import auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdDfrHNPFUkF"
      },
      "outputs": [],
      "source": [
        "#autenticação\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Autorização\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "fuso_horario = pytz.timezone('America/Sao_Paulo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RVbSVhTFhsl"
      },
      "outputs": [],
      "source": [
        "# Abrir a planilha\n",
        "planilha = gc.open(\"EP 1 - NN vs CNN - Fashion Mnist usando JAX\")\n",
        "aba = planilha.worksheet(\"Página1\")\n",
        "\n",
        "n_coluna_DATA = 1 # O número da coluna contendo a data e a hora\n",
        "n_coluna_A = 2 # Dimensão do peso 1\n",
        "n_coluna_B = 3 # Dimensão do bias 1\n",
        "n_coluna_C = 4 # Dimensão do peso 2\n",
        "n_coluna_D = 5 # Dimensão do bias 2\n",
        "n_coluna_E = 6 # Dimensão do peso 3\n",
        "n_coluna_F = 7 # Dimensão do bias 3\n",
        "n_coluna_G = 8 # Erro no treinamento\n",
        "n_coluna_H = 9 # Número de iterações\n",
        "n_coluna_I = 10 # Batch size\n",
        "n_coluna_J = 11 # Quantidade de amostras de teste\n",
        "n_coluna_K = 12 # Quantidade de amostra total\n",
        "n_coluna_L = 13 # Taxa de aprendizado\n",
        "n_coluna_M = 14 # Erro no teste\n",
        "n_coluna_N = 15 # Tempo no teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb6GpXkidJEB"
      },
      "source": [
        "## Import do Data-set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX_5QboClEaB"
      },
      "source": [
        "Usaremos o Data-set Fashion MNIST. Este dataset é composto por imagens 28x28 com escalas de cinza contendo 10 elementos de roupa. Este dataset é dividido em treino (amostras de 60k) e teste (amostra de 10k). Convertemos este dataset em JAX arrays usando modelo já construíndo no JAX. Depois, alteramos para imagens com formato (28,28,1), já que a última entrada caracteriza a classe. A rede convolucional requer canais. Como, porém, as imagens estão em escala de cinza, não temos os 3 canais RGB como imagens usualmente possuem. Para solucionar isto, adicionamos uma camada extra para a convolução. Então, normalizamos o dataset por 255 para facilitar a convergência do algoritmo de otimização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEZ1e_SzcxV8",
        "outputId": "4cb565c5-0a4b-439f-8e14-ca09bceedf31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (10000, 28, 28, 1), (60000,), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = jnp.array(X_train, dtype=jnp.float32),\\\n",
        "                                   jnp.array(X_test, dtype=jnp.float32),\\\n",
        "                                   jnp.array(Y_train, dtype=jnp.float32),\\\n",
        "                                   jnp.array(Y_test, dtype=jnp.float32)\n",
        "\n",
        "X_train, X_test = X_train.reshape(-1,28,28,1), X_test.reshape(-1,28,28,1)\n",
        "\n",
        "X_train, X_test = X_train/255.0, X_test/255.0\n",
        "\n",
        "classes =  jnp.unique(Y_train)\n",
        "\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI8ttxcStxZ3"
      },
      "source": [
        "## NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9VBKErM8ppW"
      },
      "source": [
        "### NN 1 - Rede neural com camada 784, 392, 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el-UA0XpuPyI"
      },
      "outputs": [],
      "source": [
        "neural_net_init, neural_net_apply = stax.serial(  stax.Flatten,\n",
        "                                                  stax.Dense(784),\n",
        "                                                  stax.Relu,\n",
        "                                                  stax.Dense(392),\n",
        "                                                  stax.Relu,\n",
        "                                                  stax.Dense(len(classes)),\n",
        "                                                  stax.Softmax\n",
        "                                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjPjvstQLfAd",
        "outputId": "5487fa32-3d1e-4a8b-8662-238b5426ae05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights : (784, 784), Biases : (784,)\n",
            "Weights : (784, 392), Biases : (392,)\n",
            "Weights : (392, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = neural_net_init(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1]\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCm0hsx1LpZM",
        "outputId": "1c72b56b-edc9-43ed-8de6-b1de076d851d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[0.07811955, 0.09067789, 0.09877805, 0.06826587, 0.17354672,\n",
              "        0.07074962, 0.1470148 , 0.11571854, 0.06007824, 0.0970507 ],\n",
              "       [0.12036594, 0.08408968, 0.10847562, 0.10073808, 0.14934102,\n",
              "        0.05289947, 0.11336958, 0.0693934 , 0.1203151 , 0.08101206],\n",
              "       [0.10484902, 0.08826307, 0.11507356, 0.09657142, 0.12428769,\n",
              "        0.07600646, 0.10950141, 0.09494527, 0.09451821, 0.09598389],\n",
              "       [0.1028449 , 0.08145509, 0.1158585 , 0.10533378, 0.1443685 ,\n",
              "        0.07796986, 0.10524142, 0.08857726, 0.09122137, 0.08712925],\n",
              "       [0.11193434, 0.06122527, 0.12651367, 0.12462392, 0.15348454,\n",
              "        0.05267394, 0.11282269, 0.08244407, 0.08652247, 0.08775512]],      dtype=float32)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = neural_net_apply(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVjMWL4jLxSq"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLoss(weights, input_data, actual):\n",
        "    preds = neural_net_apply(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwAJKkCdSvC0"
      },
      "outputs": [],
      "source": [
        "from jax import grad, value_and_grad\n",
        "\n",
        "def TrainModelInBatches(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLoss)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        #Update sheet only once per epoch\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((784, 784)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((784,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((784, 784)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((784,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str((784, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_H, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(learning_rate))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_I, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_J, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_L, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLoss)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_M, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_N, float(t1-t0))\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zSLYiEqTegQ",
        "outputId": "1f7c2c64-0d43-4935-d026-712e64a678f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CrossEntropyLoss : 232.788\n",
            "CrossEntropyLoss : 147.791\n",
            "CrossEntropyLoss : 131.620\n",
            "CrossEntropyLoss : 122.850\n",
            "CrossEntropyLoss : 116.936\n",
            "CrossEntropyLoss : 112.479\n",
            "CrossEntropyLoss : 108.925\n",
            "CrossEntropyLoss : 105.993\n",
            "CrossEntropyLoss : 103.481\n",
            "CrossEntropyLoss : 101.249\n",
            "CrossEntropyLoss : 99.268\n",
            "CrossEntropyLoss : 97.459\n",
            "CrossEntropyLoss : 95.813\n",
            "CrossEntropyLoss : 94.282\n",
            "CrossEntropyLoss : 92.857\n",
            "CrossEntropyLoss : 91.521\n",
            "CrossEntropyLoss : 90.262\n",
            "CrossEntropyLoss : 89.067\n",
            "CrossEntropyLoss : 87.928\n",
            "CrossEntropyLoss : 86.843\n",
            "CrossEntropyLoss : 85.800\n",
            "CrossEntropyLoss : 84.804\n",
            "CrossEntropyLoss : 83.835\n",
            "CrossEntropyLoss : 82.907\n",
            "CrossEntropyLoss : 82.007\n",
            "CrossEntropyLoss : 81.133\n",
            "CrossEntropyLoss : 80.286\n",
            "CrossEntropyLoss : 79.463\n",
            "CrossEntropyLoss : 78.666\n",
            "CrossEntropyLoss : 77.888\n",
            "CrossEntropyLoss : 77.129\n",
            "CrossEntropyLoss : 76.384\n",
            "CrossEntropyLoss : 75.659\n",
            "CrossEntropyLoss : 74.946\n",
            "CrossEntropyLoss : 74.252\n",
            "CrossEntropyLoss : 73.571\n",
            "CrossEntropyLoss : 72.901\n",
            "CrossEntropyLoss : 72.238\n",
            "CrossEntropyLoss : 71.595\n",
            "CrossEntropyLoss : 70.967\n",
            "CrossEntropyLoss : 70.342\n",
            "CrossEntropyLoss : 69.724\n",
            "CrossEntropyLoss : 69.117\n",
            "CrossEntropyLoss : 68.524\n",
            "CrossEntropyLoss : 67.942\n",
            "CrossEntropyLoss : 67.364\n",
            "CrossEntropyLoss : 66.799\n",
            "CrossEntropyLoss : 66.248\n",
            "CrossEntropyLoss : 65.694\n",
            "CrossEntropyLoss : 65.154\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 50\n",
        "batch_size=256\n",
        "\n",
        "weights = neural_net_init(rng, (batch_size,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4UQjD7OT-yo"
      },
      "outputs": [],
      "source": [
        "def MakePredictions(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        if X_batch.shape[0] != 0:\n",
        "            preds.append(neural_net_apply(weights, X_batch))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wdIhaW0UPy5",
        "outputId": "3c5ef0c2-d0b9-46a5-a2f4-024e1809b0ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Array([9, 2, 1, 1, 6], dtype=int32), Array([9, 0, 0, 3, 3], dtype=int32))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_preds = MakePredictions(opt_get_weights(final_opt_state), X_test, batch_size=batch_size)\n",
        "\n",
        "test_preds = jnp.concatenate(test_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "test_preds = jnp.argmax(test_preds, axis=1)\n",
        "\n",
        "train_preds = MakePredictions(opt_get_weights(final_opt_state), X_train, batch_size=batch_size)\n",
        "\n",
        "train_preds = jnp.concatenate(train_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "train_preds = jnp.argmax(train_preds, axis=1)\n",
        "\n",
        "test_preds[:5], train_preds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iai_4mqsWF0Y",
        "outputId": "a6bd2428-65f4-41ae-c072-8cb1dc886bf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.911\n",
            "Test  Accuracy : 0.880\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(Y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(Y_test, test_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf8DuureWLT_",
        "outputId": "69138bab-d565-40c4-d2cc-c743016ad8c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Classification Report \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.86      0.83      1000\n",
            "         1.0       0.99      0.97      0.98      1000\n",
            "         2.0       0.77      0.83      0.80      1000\n",
            "         3.0       0.84      0.92      0.88      1000\n",
            "         4.0       0.81      0.79      0.80      1000\n",
            "         5.0       0.97      0.95      0.96      1000\n",
            "         6.0       0.77      0.61      0.68      1000\n",
            "         7.0       0.93      0.95      0.94      1000\n",
            "         8.0       0.96      0.96      0.96      1000\n",
            "         9.0       0.96      0.95      0.96      1000\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Test Classification Report \")\n",
        "print(classification_report(Y_test, test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8JJ_mHaMnOA"
      },
      "source": [
        "### NN 2 - Rede neural com camada 784, 784 e 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-3Sm6e5LBFW"
      },
      "outputs": [],
      "source": [
        "neural_net2_init, neural_net2_apply = stax.serial(  stax.Flatten,\n",
        "                                                  stax.Dense(784),\n",
        "                                                  stax.Relu,\n",
        "                                                  stax.Dense(784),\n",
        "                                                  stax.Relu,\n",
        "                                                  stax.Dense(len(classes)),\n",
        "                                                  stax.Softmax\n",
        "                                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF9WWuufu8AX",
        "outputId": "d36b98c0-afca-4128-ee1a-41c7e3470b37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights : (784, 784), Biases : (784,)\n",
            "Weights : (784, 784), Biases : (784,)\n",
            "Weights : (784, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = neural_net2_init(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "#print(weights[2][0].shape)\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xUSTcrrvJUT",
        "outputId": "2653b266-c6c0-46ec-be78-10c6f7900055"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[0.07811955, 0.09067789, 0.09877805, 0.06826587, 0.17354672,\n",
              "        0.07074962, 0.1470148 , 0.11571854, 0.06007824, 0.0970507 ],\n",
              "       [0.12036594, 0.08408968, 0.10847562, 0.10073808, 0.14934102,\n",
              "        0.05289947, 0.11336958, 0.0693934 , 0.1203151 , 0.08101206],\n",
              "       [0.10484902, 0.08826307, 0.11507356, 0.09657142, 0.12428769,\n",
              "        0.07600646, 0.10950141, 0.09494527, 0.09451821, 0.09598389],\n",
              "       [0.1028449 , 0.08145509, 0.1158585 , 0.10533378, 0.1443685 ,\n",
              "        0.07796986, 0.10524142, 0.08857726, 0.09122137, 0.08712925],\n",
              "       [0.11193434, 0.06122527, 0.12651367, 0.12462392, 0.15348454,\n",
              "        0.05267394, 0.11282269, 0.08244407, 0.08652247, 0.08775512]],      dtype=float32)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = neural_net2_apply(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VtNs25qvKl8"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLoss2(weights, input_data, actual):\n",
        "    preds = neural_net2_apply(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFIEM7ZkTIe4"
      },
      "outputs": [],
      "source": [
        "from jax import grad, value_and_grad\n",
        "\n",
        "def TrainModelInBatches(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLoss2)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        #Update sheet only once per epoch\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((784, 784)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((784,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((784, 784)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((784,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str((784, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_H, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(learning_rate))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_I, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_J, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_L, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLoss2)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_M, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_N, float(t1-t0))\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTELRVv3TbTU",
        "outputId": "1f7c2c64-0d43-4935-d026-712e64a678f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CrossEntropyLoss : 232.788\n",
            "CrossEntropyLoss : 147.791\n",
            "CrossEntropyLoss : 131.620\n",
            "CrossEntropyLoss : 122.850\n",
            "CrossEntropyLoss : 116.936\n",
            "CrossEntropyLoss : 112.479\n",
            "CrossEntropyLoss : 108.925\n",
            "CrossEntropyLoss : 105.993\n",
            "CrossEntropyLoss : 103.481\n",
            "CrossEntropyLoss : 101.249\n",
            "CrossEntropyLoss : 99.268\n",
            "CrossEntropyLoss : 97.459\n",
            "CrossEntropyLoss : 95.813\n",
            "CrossEntropyLoss : 94.282\n",
            "CrossEntropyLoss : 92.857\n",
            "CrossEntropyLoss : 91.521\n",
            "CrossEntropyLoss : 90.262\n",
            "CrossEntropyLoss : 89.067\n",
            "CrossEntropyLoss : 87.928\n",
            "CrossEntropyLoss : 86.843\n",
            "CrossEntropyLoss : 85.800\n",
            "CrossEntropyLoss : 84.804\n",
            "CrossEntropyLoss : 83.835\n",
            "CrossEntropyLoss : 82.907\n",
            "CrossEntropyLoss : 82.007\n",
            "CrossEntropyLoss : 81.133\n",
            "CrossEntropyLoss : 80.286\n",
            "CrossEntropyLoss : 79.463\n",
            "CrossEntropyLoss : 78.666\n",
            "CrossEntropyLoss : 77.888\n",
            "CrossEntropyLoss : 77.129\n",
            "CrossEntropyLoss : 76.384\n",
            "CrossEntropyLoss : 75.659\n",
            "CrossEntropyLoss : 74.946\n",
            "CrossEntropyLoss : 74.252\n",
            "CrossEntropyLoss : 73.571\n",
            "CrossEntropyLoss : 72.901\n",
            "CrossEntropyLoss : 72.238\n",
            "CrossEntropyLoss : 71.595\n",
            "CrossEntropyLoss : 70.967\n",
            "CrossEntropyLoss : 70.342\n",
            "CrossEntropyLoss : 69.724\n",
            "CrossEntropyLoss : 69.117\n",
            "CrossEntropyLoss : 68.524\n",
            "CrossEntropyLoss : 67.942\n",
            "CrossEntropyLoss : 67.364\n",
            "CrossEntropyLoss : 66.799\n",
            "CrossEntropyLoss : 66.248\n",
            "CrossEntropyLoss : 65.694\n",
            "CrossEntropyLoss : 65.154\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 50\n",
        "batch_size=256\n",
        "\n",
        "weights = neural_net2_init(rng, (batch_size,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay-iITqpT5mD"
      },
      "outputs": [],
      "source": [
        "def MakePredictions2(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        if X_batch.shape[0] != 0:\n",
        "            preds.append(neural_net2_apply(weights, X_batch))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgpu8yBSUNbT",
        "outputId": "3c5ef0c2-d0b9-46a5-a2f4-024e1809b0ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Array([9, 2, 1, 1, 6], dtype=int32), Array([9, 0, 0, 3, 3], dtype=int32))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_preds = MakePredictions2(opt_get_weights(final_opt_state), X_test, batch_size=batch_size)\n",
        "\n",
        "test_preds = jnp.concatenate(test_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "test_preds = jnp.argmax(test_preds, axis=1)\n",
        "\n",
        "train_preds = MakePredictions2(opt_get_weights(final_opt_state), X_train, batch_size=batch_size)\n",
        "\n",
        "train_preds = jnp.concatenate(train_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "train_preds = jnp.argmax(train_preds, axis=1)\n",
        "\n",
        "test_preds[:5], train_preds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAOrBPWOV4u6",
        "outputId": "a6bd2428-65f4-41ae-c072-8cb1dc886bf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.911\n",
            "Test  Accuracy : 0.880\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(Y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(Y_test, test_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdn_gcw-WB_t",
        "outputId": "69138bab-d565-40c4-d2cc-c743016ad8c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Classification Report \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.86      0.83      1000\n",
            "         1.0       0.99      0.97      0.98      1000\n",
            "         2.0       0.77      0.83      0.80      1000\n",
            "         3.0       0.84      0.92      0.88      1000\n",
            "         4.0       0.81      0.79      0.80      1000\n",
            "         5.0       0.97      0.95      0.96      1000\n",
            "         6.0       0.77      0.61      0.68      1000\n",
            "         7.0       0.93      0.95      0.94      1000\n",
            "         8.0       0.96      0.96      0.96      1000\n",
            "         9.0       0.96      0.95      0.96      1000\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Test Classification Report \")\n",
        "print(classification_report(Y_test, test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p13M2YgOSTJd"
      },
      "source": [
        "### NN 3 - Rede neural com camada 12544 e 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5cTxZnLSEjr"
      },
      "outputs": [],
      "source": [
        "neural_net3_init, neural_net3_apply = stax.serial(  stax.Flatten,\n",
        "                                                  stax.Dense(12544),\n",
        "                                                  stax.Relu,\n",
        "                                                  stax.Dense(len(classes)),\n",
        "                                                  stax.Softmax\n",
        "                                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqBR-iv2SQEv",
        "outputId": "99a40143-bc8f-49d3-a4c4-5b2ab99524a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights : (784, 12544), Biases : (12544,)\n",
            "Weights : (12544, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = neural_net3_init(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsJ1l0P8uurr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDDDaFWCSegI",
        "outputId": "224f6ed8-d2e3-4821-cf8a-91df7c44a38e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[0.11567732, 0.08312271, 0.09316629, 0.13119778, 0.0849104 ,\n",
              "        0.09422207, 0.09260702, 0.10758439, 0.08504906, 0.11246294],\n",
              "       [0.12771216, 0.10170111, 0.09442902, 0.12095149, 0.07102303,\n",
              "        0.10548384, 0.09093792, 0.09384827, 0.08586559, 0.10804753],\n",
              "       [0.11517212, 0.10051443, 0.08442578, 0.10548244, 0.08962353,\n",
              "        0.10522561, 0.09384826, 0.10181855, 0.09511691, 0.10877232],\n",
              "       [0.1249938 , 0.10030809, 0.08713156, 0.10648113, 0.07983516,\n",
              "        0.10415314, 0.0952759 , 0.09500042, 0.08710919, 0.11971161],\n",
              "       [0.13157985, 0.11046299, 0.08336561, 0.12212336, 0.07734967,\n",
              "        0.10936862, 0.07990503, 0.09259556, 0.0831421 , 0.11010727]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "preds = neural_net3_apply(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO4Qc-vUSme9"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLoss3(weights, input_data, actual):\n",
        "    preds = neural_net3_apply(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEuMcZ0QxpSt"
      },
      "outputs": [],
      "source": [
        "from jax import grad, value_and_grad\n",
        "\n",
        "def TrainModelInBatches3(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLoss3)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        #Update sheet only once per epoch\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((784, 12544)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((12544,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((12544, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str())\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str())\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_H, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(learning_rate))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_I, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_J, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_L, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLoss3)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_M, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_N, float(t1-t0))\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpL-Peglx8so",
        "outputId": "02f123c7-fcee-4e02-d510-fbbb699e3d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossEntropyLoss : 222.671\n",
            "CrossEntropyLoss : 149.587\n",
            "CrossEntropyLoss : 134.210\n",
            "CrossEntropyLoss : 125.965\n",
            "CrossEntropyLoss : 120.518\n",
            "CrossEntropyLoss : 116.502\n",
            "CrossEntropyLoss : 113.332\n",
            "CrossEntropyLoss : 110.710\n",
            "CrossEntropyLoss : 108.467\n",
            "CrossEntropyLoss : 106.505\n",
            "CrossEntropyLoss : 104.756\n",
            "CrossEntropyLoss : 103.174\n",
            "CrossEntropyLoss : 101.729\n",
            "CrossEntropyLoss : 100.399\n",
            "CrossEntropyLoss : 99.163\n",
            "CrossEntropyLoss : 98.009\n",
            "CrossEntropyLoss : 96.924\n",
            "CrossEntropyLoss : 95.899\n",
            "CrossEntropyLoss : 94.929\n",
            "CrossEntropyLoss : 94.007\n",
            "CrossEntropyLoss : 93.126\n",
            "CrossEntropyLoss : 92.284\n",
            "CrossEntropyLoss : 91.476\n",
            "CrossEntropyLoss : 90.699\n",
            "CrossEntropyLoss : 89.952\n",
            "CrossEntropyLoss : 89.230\n",
            "CrossEntropyLoss : 88.531\n",
            "CrossEntropyLoss : 87.854\n",
            "CrossEntropyLoss : 87.198\n",
            "CrossEntropyLoss : 86.561\n",
            "CrossEntropyLoss : 85.941\n",
            "CrossEntropyLoss : 85.336\n",
            "CrossEntropyLoss : 84.745\n",
            "CrossEntropyLoss : 84.170\n",
            "CrossEntropyLoss : 83.607\n",
            "CrossEntropyLoss : 83.056\n",
            "CrossEntropyLoss : 82.517\n",
            "CrossEntropyLoss : 81.988\n",
            "CrossEntropyLoss : 81.470\n",
            "CrossEntropyLoss : 80.961\n",
            "CrossEntropyLoss : 80.463\n",
            "CrossEntropyLoss : 79.972\n",
            "CrossEntropyLoss : 79.491\n",
            "CrossEntropyLoss : 79.016\n",
            "CrossEntropyLoss : 78.550\n",
            "CrossEntropyLoss : 78.090\n",
            "CrossEntropyLoss : 77.638\n",
            "CrossEntropyLoss : 77.194\n",
            "CrossEntropyLoss : 76.756\n",
            "CrossEntropyLoss : 76.325\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 50\n",
        "batch_size=256\n",
        "\n",
        "weights = neural_net3_init(rng, (batch_size,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches3(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hNhts4bXfSx"
      },
      "outputs": [],
      "source": [
        "def MakePredictions3(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        if X_batch.shape[0] != 0:\n",
        "            preds.append(neural_net3_apply(weights, X_batch))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uKcPoICX237",
        "outputId": "e5c471a1-8e08-49ee-9643-818fcdefb9e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([9, 2, 1, 1, 6], dtype=int32), Array([9, 0, 0, 3, 3], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "test_preds = MakePredictions3(opt_get_weights(final_opt_state), X_test, batch_size=batch_size)\n",
        "\n",
        "test_preds = jnp.concatenate(test_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "test_preds = jnp.argmax(test_preds, axis=1)\n",
        "\n",
        "train_preds = MakePredictions3(opt_get_weights(final_opt_state), X_train, batch_size=batch_size)\n",
        "\n",
        "train_preds = jnp.concatenate(train_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "train_preds = jnp.argmax(train_preds, axis=1)\n",
        "\n",
        "test_preds[:5], train_preds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtB2RHknYW1N",
        "outputId": "d0d58f72-cc5f-44cc-edb8-d3d4047f6eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.897\n",
            "Test  Accuracy : 0.872\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(Y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(Y_test, test_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nLXORKTYoGK",
        "outputId": "57c822cb-5baa-4fc9-da86-f5fc34a7b607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Classification Report \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.87      0.83      1000\n",
            "         1.0       0.98      0.96      0.97      1000\n",
            "         2.0       0.75      0.82      0.79      1000\n",
            "         3.0       0.83      0.90      0.87      1000\n",
            "         4.0       0.80      0.79      0.79      1000\n",
            "         5.0       0.96      0.95      0.95      1000\n",
            "         6.0       0.77      0.57      0.66      1000\n",
            "         7.0       0.93      0.93      0.93      1000\n",
            "         8.0       0.95      0.96      0.96      1000\n",
            "         9.0       0.94      0.96      0.95      1000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Test Classification Report \")\n",
        "print(classification_report(Y_test, test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NN 4 - Rede neural com camada 784 e 10"
      ],
      "metadata": {
        "id": "QC843fAE09hS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4aWt2HB1KtL"
      },
      "outputs": [],
      "source": [
        "neural_net4_init, neural_net4_apply = stax.serial(  stax.Flatten,\n",
        "                                                  stax.Dense(784),\n",
        "                                                  stax.Relu,\n",
        "                                                  stax.Dense(len(classes)),\n",
        "                                                  stax.Softmax\n",
        "                                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0f7c9e-4026-4383-bcbd-c87bd025e627",
        "id": "f4mol5KQ1KtM"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights : (784, 784), Biases : (784,)\n",
            "Weights : (784, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = neural_net4_init(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b532d3c1-7cf6-4785-ccb1-efec11dcd19b",
        "id": "OZuzB3sE1KtM"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[0.09612467, 0.10410833, 0.18388611, 0.12166529, 0.05448725,\n",
              "        0.08495695, 0.06539526, 0.07924715, 0.06066569, 0.14946328],\n",
              "       [0.1347176 , 0.14377116, 0.13551466, 0.08447529, 0.04370638,\n",
              "        0.10342203, 0.02061974, 0.09367398, 0.08857248, 0.15152664],\n",
              "       [0.1026377 , 0.1213987 , 0.12713546, 0.07150632, 0.07939307,\n",
              "        0.09092852, 0.08071359, 0.11131992, 0.08349042, 0.13147631],\n",
              "       [0.10809993, 0.15784532, 0.16631818, 0.05506259, 0.06262441,\n",
              "        0.0834934 , 0.06314867, 0.11416489, 0.06958278, 0.11965981],\n",
              "       [0.08680553, 0.1079725 , 0.19477277, 0.05983886, 0.05173325,\n",
              "        0.10810154, 0.04337629, 0.1107651 , 0.06790921, 0.16872497]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "preds = neural_net4_apply(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g1ogo9z1KtM"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLoss4(weights, input_data, actual):\n",
        "    preds = neural_net4_apply(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wM9SFIS1KtM"
      },
      "outputs": [],
      "source": [
        "from jax import grad, value_and_grad\n",
        "\n",
        "def TrainModelInBatches4(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLoss4)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        #Update sheet only once per epoch\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((784, 784)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((784,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((784, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str())\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str())\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_H, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(learning_rate))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_I, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_J, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_L, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLoss4)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_M, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_N, float(t1-t0))\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c53090-1697-4913-cdf4-d77899f06127",
        "id": "IS0oIZf-1KtN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossEntropyLoss : 238.141\n",
            "CrossEntropyLoss : 155.597\n",
            "CrossEntropyLoss : 138.593\n",
            "CrossEntropyLoss : 129.665\n",
            "CrossEntropyLoss : 123.839\n",
            "CrossEntropyLoss : 119.607\n",
            "CrossEntropyLoss : 116.311\n",
            "CrossEntropyLoss : 113.612\n",
            "CrossEntropyLoss : 111.314\n",
            "CrossEntropyLoss : 109.304\n",
            "CrossEntropyLoss : 107.531\n",
            "CrossEntropyLoss : 105.940\n",
            "CrossEntropyLoss : 104.491\n",
            "CrossEntropyLoss : 103.154\n",
            "CrossEntropyLoss : 101.911\n",
            "CrossEntropyLoss : 100.749\n",
            "CrossEntropyLoss : 99.658\n",
            "CrossEntropyLoss : 98.627\n",
            "CrossEntropyLoss : 97.648\n",
            "CrossEntropyLoss : 96.723\n",
            "CrossEntropyLoss : 95.839\n",
            "CrossEntropyLoss : 94.994\n",
            "CrossEntropyLoss : 94.184\n",
            "CrossEntropyLoss : 93.410\n",
            "CrossEntropyLoss : 92.667\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 25\n",
        "batch_size=256\n",
        "\n",
        "weights = neural_net4_init(rng, (batch_size,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches4(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMwyxSvW1KtN"
      },
      "outputs": [],
      "source": [
        "def MakePredictions4(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        if X_batch.shape[0] != 0:\n",
        "            preds.append(neural_net4_apply(weights, X_batch))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ee9d08-83e2-4fda-ba26-54487d0c890c",
        "id": "vPhmSzGS1KtN"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([9, 2, 1, 1, 6], dtype=int32), Array([9, 0, 0, 0, 3], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "test_preds = MakePredictions4(opt_get_weights(final_opt_state), X_test, batch_size=batch_size)\n",
        "\n",
        "test_preds = jnp.concatenate(test_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "test_preds = jnp.argmax(test_preds, axis=1)\n",
        "\n",
        "train_preds = MakePredictions4(opt_get_weights(final_opt_state), X_train, batch_size=batch_size)\n",
        "\n",
        "train_preds = jnp.concatenate(train_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "train_preds = jnp.argmax(train_preds, axis=1)\n",
        "\n",
        "test_preds[:5], train_preds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "216ee7ac-d6c3-410f-afb5-84c3e6fb0c21",
        "id": "DGWANfug1KtO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.876\n",
            "Test  Accuracy : 0.857\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(Y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(Y_test, test_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e6b31a-89fb-47ef-bd9b-4c5c4169fd8b",
        "id": "2Tjy3UJ61KtO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Classification Report \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.85      0.82      1000\n",
            "         1.0       0.98      0.96      0.97      1000\n",
            "         2.0       0.73      0.81      0.77      1000\n",
            "         3.0       0.82      0.89      0.86      1000\n",
            "         4.0       0.77      0.76      0.76      1000\n",
            "         5.0       0.95      0.93      0.94      1000\n",
            "         6.0       0.73      0.54      0.62      1000\n",
            "         7.0       0.92      0.92      0.92      1000\n",
            "         8.0       0.94      0.95      0.94      1000\n",
            "         9.0       0.94      0.96      0.95      1000\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.85     10000\n",
            "weighted avg       0.86      0.86      0.85     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Test Classification Report \")\n",
        "print(classification_report(Y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDrNX1VBBl9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (EXTRA) NN 5 - Rede neural com camada 12544, 12544, 12544 e 10\n",
        "O que aconteceria se tivemos uma super NN?\n"
      ],
      "metadata": {
        "id": "DYjVHgepBmJE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJzaKIwhBmJE"
      },
      "outputs": [],
      "source": [
        "neural_net5_init, neural_net5_apply = stax.serial(  stax.Flatten,\n",
        "                                                  stax.Dense(12544),\n",
        "                                                  stax.Relu,\n",
        "                                                  stax.Dense(12544),\n",
        "                                                  stax.Relu,\n",
        "                                                  stax.Dense(12544),\n",
        "                                                  stax.Relu,\n",
        "                                                  stax.Dense(len(classes)),\n",
        "                                                  stax.Softmax\n",
        "                                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21836d8a-93e2-4a90-9098-fa270c165cfb",
        "id": "vGj0VFNLBmJF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights : (784, 12544), Biases : (12544,)\n",
            "Weights : (12544, 12544), Biases : (12544,)\n",
            "Weights : (12544, 12544), Biases : (12544,)\n",
            "Weights : (12544, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = neural_net5_init(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a896e0a-6102-4c64-d4bd-d40d032f6231",
        "id": "bhoXeetpBmJF"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[0.09783493, 0.09493916, 0.08850034, 0.11536992, 0.10089111,\n",
              "        0.09155177, 0.11366444, 0.10272201, 0.09576342, 0.09876294],\n",
              "       [0.09001359, 0.09726787, 0.0983184 , 0.09964491, 0.11205891,\n",
              "        0.09315657, 0.11798366, 0.0958628 , 0.09672335, 0.09896995],\n",
              "       [0.09709807, 0.09883974, 0.09811977, 0.10343327, 0.10358185,\n",
              "        0.09406121, 0.10775121, 0.10162062, 0.09505739, 0.10043693],\n",
              "       [0.09554135, 0.10430618, 0.10017609, 0.10546475, 0.10143921,\n",
              "        0.09168094, 0.1080877 , 0.09815247, 0.09263518, 0.10251615],\n",
              "       [0.08937951, 0.10172193, 0.09391555, 0.10992814, 0.10593214,\n",
              "        0.09310725, 0.11358219, 0.10120934, 0.09769433, 0.09352971]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "preds = neural_net5_apply(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvHthKVOBmJF"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLoss5(weights, input_data, actual):\n",
        "    preds = neural_net5_apply(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw7El_cfBmJF"
      },
      "outputs": [],
      "source": [
        "from jax import grad, value_and_grad\n",
        "\n",
        "def TrainModelInBatches5(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLoss5)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        #Update sheet only once per epoch\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((784, 784)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((784,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((784, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str())\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str())\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_H, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(learning_rate))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_I, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_J, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_L, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLoss5)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_M, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_N, float(t1-t0))\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e11a99-f176-441c-f45e-3ed7871e7362",
        "id": "kF3gflMwBmJG"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossEntropyLoss : 219.660\n",
            "CrossEntropyLoss : 139.220\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 25\n",
        "batch_size=256\n",
        "\n",
        "weights = neural_net5_init(rng, (batch_size,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches5(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resposta: deu falha de memória RAM."
      ],
      "metadata": {
        "id": "WLQEmmOLsfU-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va3bbMMUde69"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe0Y8TR8pvRP"
      },
      "source": [
        "### CNN 1 - Conv+RELu 32, Conv+RELu 16 e Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg1g6TJZc35S"
      },
      "outputs": [],
      "source": [
        "conv_init, conv_apply = stax.serial(\n",
        "    stax.Conv(32,(3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.Conv(16, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "\n",
        "    stax.Flatten,\n",
        "    stax.Dense(len(classes)),\n",
        "    stax.Softmax\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY9O_h64XHEP"
      },
      "outputs": [],
      "source": [
        "# Abrir a planilha\n",
        "planilha = gc.open(\"EP 1 - NN vs CNN - Fashion Mnist usando JAX\")\n",
        "aba = planilha.worksheet(\"Página2\")\n",
        "\n",
        "n_coluna_DATA = 1 # O número da coluna contendo a data e a hora\n",
        "n_coluna_A = 2 # Dimensão do peso 1\n",
        "n_coluna_B = 3 # Dimensão do bibias 1\n",
        "n_coluna_C = 4 # Dimensão do peso 2\n",
        "n_coluna_D = 5 # Dimensão do bias 2\n",
        "n_coluna_E = 6 # Dimensão do peso 3\n",
        "n_coluna_F = 7 # Dimensão do bias 3\n",
        "n_coluna_G = 8 # Dimensão do peso 4\n",
        "n_coluna_H = 9 # Dimensão do bias 4\n",
        "n_coluna_I = 10 # Erro no treinamento\n",
        "n_coluna_J = 11 # Número de iterações\n",
        "n_coluna_K = 12 # Batch size\n",
        "n_coluna_L = 13 # Quantidade de amostras de teste\n",
        "n_coluna_M = 14 # Quantidade de amostra total\n",
        "n_coluna_N = 15 # Taxa de aprendizado\n",
        "n_coluna_O = 16 # Erro no teste\n",
        "n_coluna_P = 17 # Tempo no teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfz3QPXlXUsl",
        "outputId": "b9fb3a33-6b7d-43ed-b005-db59b61c9925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights : (3, 3, 1, 32), Biases : (1, 1, 1, 32)\n",
            "Weights : (3, 3, 32, 16), Biases : (1, 1, 1, 16)\n",
            "Weights : (12544, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = conv_init(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVn47KurXfC5",
        "outputId": "6070f534-db0c-4c96-cdbb-c4cdd75d0674"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[0.1010583 , 0.08450612, 0.08876862, 0.10357413, 0.09424469,\n",
              "        0.06837884, 0.13585268, 0.10559722, 0.10213768, 0.11588172],\n",
              "       [0.10652137, 0.08236858, 0.11018915, 0.11165013, 0.08258918,\n",
              "        0.07724519, 0.14238866, 0.09115906, 0.0884075 , 0.10748117],\n",
              "       [0.09566505, 0.09545239, 0.10094573, 0.10249694, 0.09882495,\n",
              "        0.08883353, 0.11344208, 0.09690957, 0.10275006, 0.10467971],\n",
              "       [0.10200435, 0.08598676, 0.10633808, 0.10407417, 0.09481844,\n",
              "        0.0829622 , 0.12135128, 0.093064  , 0.09924014, 0.11016052],\n",
              "       [0.09390713, 0.08359886, 0.10012674, 0.11463808, 0.09753538,\n",
              "        0.07206484, 0.12989205, 0.08960547, 0.10824842, 0.11038304]],      dtype=float32)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = conv_apply(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYn4sSBJXoAy"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLossConv(weights, input_data, actual):\n",
        "    preds = conv_apply(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsPWwD8fXy_E"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "def TrainModelInBatches(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLossConv)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((3, 3, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((1, 1, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((3, 3, 32, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((1, 1, 1, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str((3, 3, 16, 8)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str((1, 1, 1, 8)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, str((6272, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_H, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_I, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_J, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_L, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_M, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_N, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLossConv)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_O, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_P, float(t1-t0))\n",
        "\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJDp2vUaX92l",
        "outputId": "28129b02-fd31-4f15-f09b-254109f5a3c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CrossEntropyLoss : 237.197\n",
            "CrossEntropyLoss : 137.667\n",
            "CrossEntropyLoss : 119.940\n",
            "CrossEntropyLoss : 110.607\n",
            "CrossEntropyLoss : 104.348\n",
            "CrossEntropyLoss : 99.563\n",
            "CrossEntropyLoss : 95.596\n",
            "CrossEntropyLoss : 92.126\n",
            "CrossEntropyLoss : 89.062\n",
            "CrossEntropyLoss : 86.342\n",
            "CrossEntropyLoss : 83.902\n",
            "CrossEntropyLoss : 81.713\n",
            "CrossEntropyLoss : 79.717\n",
            "CrossEntropyLoss : 77.903\n",
            "CrossEntropyLoss : 76.245\n",
            "CrossEntropyLoss : 74.700\n",
            "CrossEntropyLoss : 73.277\n",
            "CrossEntropyLoss : 71.944\n",
            "CrossEntropyLoss : 70.720\n",
            "CrossEntropyLoss : 69.546\n",
            "CrossEntropyLoss : 68.426\n",
            "CrossEntropyLoss : 67.369\n",
            "CrossEntropyLoss : 66.337\n",
            "CrossEntropyLoss : 65.365\n",
            "CrossEntropyLoss : 64.449\n",
            "CrossEntropyLoss : 63.565\n",
            "CrossEntropyLoss : 62.700\n",
            "CrossEntropyLoss : 61.857\n",
            "CrossEntropyLoss : 61.050\n",
            "CrossEntropyLoss : 60.265\n",
            "CrossEntropyLoss : 59.496\n",
            "CrossEntropyLoss : 58.760\n",
            "CrossEntropyLoss : 58.045\n",
            "CrossEntropyLoss : 57.351\n",
            "CrossEntropyLoss : 56.662\n",
            "CrossEntropyLoss : 56.007\n",
            "CrossEntropyLoss : 55.362\n",
            "CrossEntropyLoss : 54.738\n",
            "CrossEntropyLoss : 54.139\n",
            "CrossEntropyLoss : 53.539\n",
            "CrossEntropyLoss : 52.963\n",
            "CrossEntropyLoss : 52.396\n",
            "CrossEntropyLoss : 51.836\n",
            "CrossEntropyLoss : 51.284\n",
            "CrossEntropyLoss : 50.741\n",
            "CrossEntropyLoss : 50.202\n",
            "CrossEntropyLoss : 49.680\n",
            "CrossEntropyLoss : 49.164\n",
            "CrossEntropyLoss : 48.653\n",
            "CrossEntropyLoss : 48.158\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 50\n",
        "batch_size=256\n",
        "\n",
        "weights = conv_init(rng, (18,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3gruxgzZTCb"
      },
      "outputs": [],
      "source": [
        "def MakePredictions(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        if X_batch.shape[0] != 0:\n",
        "            preds.append(conv_apply(weights, X_batch))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wGfx6VXZm3N",
        "outputId": "e1f15f42-4b20-4f29-bf41-e59f86683de9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Array([9, 2, 1, 1, 6], dtype=int32), Array([9, 0, 0, 3, 0], dtype=int32))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_preds = MakePredictions(opt_get_weights(final_opt_state), X_test, batch_size=batch_size)\n",
        "\n",
        "test_preds = jnp.concatenate(test_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "test_preds = jnp.argmax(test_preds, axis=1)\n",
        "\n",
        "train_preds = MakePredictions(opt_get_weights(final_opt_state), X_train, batch_size=batch_size)\n",
        "\n",
        "train_preds = jnp.concatenate(train_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "train_preds = jnp.argmax(train_preds, axis=1)\n",
        "\n",
        "test_preds[:5], train_preds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxvoH9qiZ0jT",
        "outputId": "72d8cf7c-022e-48a8-f98b-c6a1ee38a81a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.934\n",
            "Test  Accuracy : 0.898\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(Y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(Y_test, test_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOekexF3Z3nN",
        "outputId": "1b26d697-d660-4542-a54e-f563b4f75c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Classification Report \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.85      0.84      1000\n",
            "         1.0       0.99      0.97      0.98      1000\n",
            "         2.0       0.83      0.86      0.85      1000\n",
            "         3.0       0.87      0.93      0.90      1000\n",
            "         4.0       0.88      0.77      0.82      1000\n",
            "         5.0       0.98      0.98      0.98      1000\n",
            "         6.0       0.71      0.73      0.72      1000\n",
            "         7.0       0.95      0.96      0.96      1000\n",
            "         8.0       0.98      0.97      0.98      1000\n",
            "         9.0       0.97      0.96      0.96      1000\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Test Classification Report \")\n",
        "print(classification_report(Y_test, test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aRhfcN6WhQq"
      },
      "source": [
        "### CNN 2 - Conv+RELu 32, Conv+RELu 16, Conv 8, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L1BzNYCU08T"
      },
      "outputs": [],
      "source": [
        "conv_init2, conv_apply2 = stax.serial(\n",
        "    stax.Conv(32,(3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.Conv(16, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.Conv(8, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "\n",
        "    stax.Flatten,\n",
        "    stax.Dense(len(classes)),\n",
        "    stax.Softmax\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFHuDv4R5j-f"
      },
      "outputs": [],
      "source": [
        "# Abrir a planilha\n",
        "planilha = gc.open(\"EP 1 - NN vs CNN - Fashion Mnist usando JAX\")\n",
        "aba = planilha.worksheet(\"Página2\")\n",
        "\n",
        "n_coluna_DATA = 1 # O número da coluna contendo a data e a hora\n",
        "n_coluna_A = 2 # Dimensão do peso 1\n",
        "n_coluna_B = 3 # Dimensão do bibias 1\n",
        "n_coluna_C = 4 # Dimensão do peso 2\n",
        "n_coluna_D = 5 # Dimensão do bias 2\n",
        "n_coluna_E = 6 # Dimensão do peso 3\n",
        "n_coluna_F = 7 # Dimensão do bias 3\n",
        "n_coluna_G = 8 # Dimensão do peso 4\n",
        "n_coluna_H = 9 # Dimensão do bias 4\n",
        "n_coluna_I = 10 # Erro no treinamento\n",
        "n_coluna_J = 11 # Número de iterações\n",
        "n_coluna_K = 12 # Batch size\n",
        "n_coluna_L = 13 # Quantidade de amostras de teste\n",
        "n_coluna_M = 14 # Quantidade de amostra total\n",
        "n_coluna_N = 15 # Taxa de aprendizado\n",
        "n_coluna_O = 16 # Erro no teste\n",
        "n_coluna_P = 17 # Tempo no teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UaZAlkTVGFN",
        "outputId": "99fc8ae2-19fe-47ec-e2cc-149b538c77cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights : (3, 3, 1, 32), Biases : (1, 1, 1, 32)\n",
            "Weights : (3, 3, 32, 16), Biases : (1, 1, 1, 16)\n",
            "Weights : (3, 3, 16, 8), Biases : (1, 1, 1, 8)\n",
            "Weights : (6272, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = conv_init2(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D0SuAwkdtDL",
        "outputId": "b58b1f22-6565-4c9d-eae0-10f5b7303e39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[0.11038224, 0.10035543, 0.09388019, 0.09824558, 0.09850648,\n",
              "        0.10333223, 0.10351744, 0.0947985 , 0.09528428, 0.10169768],\n",
              "       [0.11012788, 0.09846697, 0.09884238, 0.10509334, 0.09760616,\n",
              "        0.09469733, 0.10269596, 0.09468425, 0.09779315, 0.0999926 ],\n",
              "       [0.10290439, 0.10052056, 0.09763322, 0.10209966, 0.09708749,\n",
              "        0.10292315, 0.1029613 , 0.09659406, 0.09772114, 0.09955501],\n",
              "       [0.10490854, 0.09879624, 0.09545968, 0.10218364, 0.09938176,\n",
              "        0.10210045, 0.10189226, 0.09747745, 0.09721155, 0.10058835],\n",
              "       [0.11069517, 0.10137364, 0.08963691, 0.10273396, 0.09639259,\n",
              "        0.10485574, 0.10414395, 0.09456155, 0.09492914, 0.10067735]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "preds = conv_apply2(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJRXXKiIdyZs"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLossConv2(weights, input_data, actual):\n",
        "    preds = conv_apply2(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrdGz-GJd-9A"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "def TrainModelInBatches(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLossConv2)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((3, 3, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((1, 1, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((3, 3, 32, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((1, 1, 1, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str((3, 3, 16, 8)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str((1, 1, 1, 8)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, str((6272, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_H, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_I, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_J, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_L, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_M, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_N, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLossConv2)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_O, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_P, float(t1-t0))\n",
        "\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3miS1Z9eHKM",
        "outputId": "54d774e6-f57d-4c18-ad32-76c6b13e3553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossEntropyLoss : 237.197\n",
            "CrossEntropyLoss : 137.667\n",
            "CrossEntropyLoss : 119.940\n",
            "CrossEntropyLoss : 110.607\n",
            "CrossEntropyLoss : 104.348\n",
            "CrossEntropyLoss : 99.563\n",
            "CrossEntropyLoss : 95.596\n",
            "CrossEntropyLoss : 92.126\n",
            "CrossEntropyLoss : 89.062\n",
            "CrossEntropyLoss : 86.342\n",
            "CrossEntropyLoss : 83.902\n",
            "CrossEntropyLoss : 81.713\n",
            "CrossEntropyLoss : 79.717\n",
            "CrossEntropyLoss : 77.903\n",
            "CrossEntropyLoss : 76.245\n",
            "CrossEntropyLoss : 74.700\n",
            "CrossEntropyLoss : 73.277\n",
            "CrossEntropyLoss : 71.944\n",
            "CrossEntropyLoss : 70.720\n",
            "CrossEntropyLoss : 69.546\n",
            "CrossEntropyLoss : 68.426\n",
            "CrossEntropyLoss : 67.369\n",
            "CrossEntropyLoss : 66.337\n",
            "CrossEntropyLoss : 65.365\n",
            "CrossEntropyLoss : 64.449\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 25\n",
        "batch_size=256\n",
        "\n",
        "weights = conv_init2(rng, (18,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIq2wqxieLWT"
      },
      "outputs": [],
      "source": [
        "def MakePredictions2(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        if X_batch.shape[0] != 0:\n",
        "            preds.append(conv_apply2(weights, X_batch))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YRx5L-hslMh",
        "outputId": "89e54b95-c269-417e-e9ff-cb8f23ab1ce9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([9, 2, 1, 1, 6], dtype=int32), Array([9, 0, 0, 3, 0], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "test_preds = MakePredictions2(opt_get_weights(final_opt_state), X_test, batch_size=batch_size)\n",
        "\n",
        "test_preds = jnp.concatenate(test_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "test_preds = jnp.argmax(test_preds, axis=1)\n",
        "\n",
        "train_preds = MakePredictions2(opt_get_weights(final_opt_state), X_train, batch_size=batch_size)\n",
        "\n",
        "train_preds = jnp.concatenate(train_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "train_preds = jnp.argmax(train_preds, axis=1)\n",
        "\n",
        "test_preds[:5], train_preds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0u_p-3nswpN",
        "outputId": "292ec705-651f-4667-831e-d1d6db4147fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.910\n",
            "Test  Accuracy : 0.887\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(Y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(Y_test, test_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ROgTtMgs3Xn",
        "outputId": "b2544171-f9d0-404a-9e5c-55598e0fe4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Classification Report \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.86      0.84      1000\n",
            "         1.0       0.99      0.96      0.98      1000\n",
            "         2.0       0.77      0.88      0.82      1000\n",
            "         3.0       0.86      0.93      0.89      1000\n",
            "         4.0       0.87      0.72      0.79      1000\n",
            "         5.0       0.98      0.97      0.97      1000\n",
            "         6.0       0.73      0.66      0.69      1000\n",
            "         7.0       0.94      0.97      0.95      1000\n",
            "         8.0       0.98      0.97      0.97      1000\n",
            "         9.0       0.97      0.96      0.96      1000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Test Classification Report \")\n",
        "print(classification_report(Y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUx6qH2Xs6Ga"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl5u4PFeZg28"
      },
      "source": [
        "### CNN 3 - Conv32 + RELu, AvgPool(3,3), Conv16+RELu, AvgPool(3,3), Flatten e Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqfG118DdS9i"
      },
      "outputs": [],
      "source": [
        "conv_init3, conv_apply3 = stax.serial(\n",
        "    stax.Conv(32,(3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.AvgPool((3,3), padding=\"SAME\"),\n",
        "    stax.Conv(16, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.AvgPool((3,3), padding=\"SAME\"),\n",
        "\n",
        "    stax.Flatten,\n",
        "    stax.Dense(len(classes)),\n",
        "    stax.Softmax\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtMAyVFYgNmG"
      },
      "outputs": [],
      "source": [
        "# Abrir a planilha\n",
        "planilha = gc.open(\"EP 1 - NN vs CNN - Fashion Mnist usando JAX\")\n",
        "aba = planilha.worksheet(\"Página2\")\n",
        "\n",
        "n_coluna_DATA = 1 # O número da coluna contendo a data e a hora\n",
        "n_coluna_A = 2 # Dimensão do peso 1\n",
        "n_coluna_B = 3 # Dimensão do bibias 1\n",
        "n_coluna_C = 4 # Dimensão do peso 2\n",
        "n_coluna_D = 5 # Dimensão do bias 2\n",
        "n_coluna_E = 6 # Dimensão do peso 3\n",
        "n_coluna_F = 7 # Dimensão do bias 3\n",
        "n_coluna_G = 8 # Dimensão do peso 4\n",
        "n_coluna_H = 9 # Dimensão do bias 4\n",
        "n_coluna_I = 10 # Erro no treinamento\n",
        "n_coluna_J = 11 # Número de iterações\n",
        "n_coluna_K = 12 # Batch size\n",
        "n_coluna_L = 13 # Quantidade de amostras de teste\n",
        "n_coluna_M = 14 # Quantidade de amostra total\n",
        "n_coluna_N = 15 # Taxa de aprendizado\n",
        "n_coluna_O = 16 # Erro no teste\n",
        "n_coluna_P = 17 # Tempo no teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnR85rC2hVNh",
        "outputId": "8d34917c-2f85-4350-8c7d-e2cc6a81d9cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights : (3, 3, 1, 32), Biases : (1, 1, 1, 32)\n",
            "Weights : (3, 3, 32, 16), Biases : (1, 1, 1, 16)\n",
            "Weights : (12544, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = conv_init3(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSUMQtNnh2NL",
        "outputId": "00cfe7e1-2d68-4339-bc4e-089ad0fdc8a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[0.09840897, 0.11047231, 0.11040694, 0.10387599, 0.0970936 ,\n",
              "        0.09818683, 0.09109973, 0.09893171, 0.10123079, 0.09029309],\n",
              "       [0.09430714, 0.10524321, 0.09991619, 0.10411722, 0.09135789,\n",
              "        0.09486455, 0.10155654, 0.09576621, 0.10513603, 0.10773499],\n",
              "       [0.09452868, 0.10610519, 0.09852562, 0.10138861, 0.09767579,\n",
              "        0.10012037, 0.10115271, 0.10077183, 0.10145459, 0.09827655],\n",
              "       [0.09371365, 0.10674328, 0.09712839, 0.10316705, 0.09963594,\n",
              "        0.09866244, 0.10053799, 0.10001117, 0.09940703, 0.100993  ],\n",
              "       [0.09558828, 0.11103137, 0.09993856, 0.10479355, 0.09826652,\n",
              "        0.09980534, 0.10059945, 0.0988933 , 0.09651057, 0.09457301]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "preds = conv_apply3(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLGfzKTWh8nq"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLossConv3(weights, input_data, actual):\n",
        "    preds = conv_apply3(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLCN3UaQiGc-"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "def TrainModelInBatches(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLossConv3)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((3, 3, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((1, 1, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((3,3)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((3, 3, 32, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str((1, 1, 1, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str((3,3)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, str((12544, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_H, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_I, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_J, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_L, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_M, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_N, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLossConv3)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_O, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_P, float(t1-t0))\n",
        "\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0dZ54Yoifko",
        "outputId": "0893e131-d3ac-4d92-d0d4-161f104e28bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossEntropyLoss : 248.126\n",
            "CrossEntropyLoss : 166.875\n",
            "CrossEntropyLoss : 147.230\n",
            "CrossEntropyLoss : 134.101\n",
            "CrossEntropyLoss : 124.303\n",
            "CrossEntropyLoss : 116.747\n",
            "CrossEntropyLoss : 111.112\n",
            "CrossEntropyLoss : 106.854\n",
            "CrossEntropyLoss : 103.467\n",
            "CrossEntropyLoss : 100.647\n",
            "CrossEntropyLoss : 98.252\n",
            "CrossEntropyLoss : 96.194\n",
            "CrossEntropyLoss : 94.386\n",
            "CrossEntropyLoss : 92.776\n",
            "CrossEntropyLoss : 91.345\n",
            "CrossEntropyLoss : 90.026\n",
            "CrossEntropyLoss : 88.820\n",
            "CrossEntropyLoss : 87.711\n",
            "CrossEntropyLoss : 86.662\n",
            "CrossEntropyLoss : 85.675\n",
            "CrossEntropyLoss : 84.758\n",
            "CrossEntropyLoss : 83.877\n",
            "CrossEntropyLoss : 83.038\n",
            "CrossEntropyLoss : 82.242\n",
            "CrossEntropyLoss : 81.490\n",
            "CrossEntropyLoss : 80.761\n",
            "CrossEntropyLoss : 80.062\n",
            "CrossEntropyLoss : 79.376\n",
            "CrossEntropyLoss : 78.715\n",
            "CrossEntropyLoss : 78.074\n",
            "CrossEntropyLoss : 77.455\n",
            "CrossEntropyLoss : 76.858\n",
            "CrossEntropyLoss : 76.276\n",
            "CrossEntropyLoss : 75.706\n",
            "CrossEntropyLoss : 75.160\n",
            "CrossEntropyLoss : 74.625\n",
            "CrossEntropyLoss : 74.104\n",
            "CrossEntropyLoss : 73.595\n",
            "CrossEntropyLoss : 73.094\n",
            "CrossEntropyLoss : 72.614\n",
            "CrossEntropyLoss : 72.131\n",
            "CrossEntropyLoss : 71.658\n",
            "CrossEntropyLoss : 71.194\n",
            "CrossEntropyLoss : 70.755\n",
            "CrossEntropyLoss : 70.316\n",
            "CrossEntropyLoss : 69.893\n",
            "CrossEntropyLoss : 69.474\n",
            "CrossEntropyLoss : 69.054\n",
            "CrossEntropyLoss : 68.655\n",
            "CrossEntropyLoss : 68.257\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 50\n",
        "batch_size=256\n",
        "\n",
        "weights = conv_init3(rng, (18,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLls574Pt4qG"
      },
      "outputs": [],
      "source": [
        "def MakePredictions3(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        if X_batch.shape[0] != 0:\n",
        "            preds.append(conv_apply3(weights, X_batch))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBZW8nnhuBDL",
        "outputId": "5a19b7fe-944d-4419-a744-48c2f7507ad4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([9, 2, 1, 1, 6], dtype=int32), Array([9, 0, 0, 3, 3], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "test_preds = MakePredictions3(opt_get_weights(final_opt_state), X_test, batch_size=batch_size)\n",
        "\n",
        "test_preds = jnp.concatenate(test_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "test_preds = jnp.argmax(test_preds, axis=1)\n",
        "\n",
        "train_preds = MakePredictions3(opt_get_weights(final_opt_state), X_train, batch_size=batch_size)\n",
        "\n",
        "train_preds = jnp.concatenate(train_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "train_preds = jnp.argmax(train_preds, axis=1)\n",
        "\n",
        "test_preds[:5], train_preds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_jMnZgAuHPW",
        "outputId": "00956472-87f9-48fa-8c79-e9bdbde4199b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.910\n",
            "Test  Accuracy : 0.896\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(Y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(Y_test, test_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvvvhtMDuLv_",
        "outputId": "65e11577-a3cb-4da9-a159-0864cb585d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Classification Report \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.85      0.84      1000\n",
            "         1.0       0.99      0.96      0.98      1000\n",
            "         2.0       0.85      0.83      0.84      1000\n",
            "         3.0       0.86      0.92      0.89      1000\n",
            "         4.0       0.84      0.84      0.84      1000\n",
            "         5.0       0.98      0.97      0.98      1000\n",
            "         6.0       0.73      0.69      0.71      1000\n",
            "         7.0       0.94      0.96      0.95      1000\n",
            "         8.0       0.97      0.97      0.97      1000\n",
            "         9.0       0.96      0.96      0.96      1000\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Test Classification Report \")\n",
        "print(classification_report(Y_test, test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN 4 -Conv32+RELu, MaxPool, Conv16+RELu, MaxPool, Flatten"
      ],
      "metadata": {
        "id": "WMdDNFCXYZSs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJCFqdhZhVeZ"
      },
      "outputs": [],
      "source": [
        "conv_init4, conv_apply4 = stax.serial(\n",
        "stax.Conv(32,(3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool(window_shape=(3,3), strides=(2,2), padding=\"SAME\"),\n",
        "    stax.Conv(16, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool(window_shape=(3,3), strides=(2,2), padding=\"SAME\"),\n",
        "\n",
        "    stax.Flatten,\n",
        "    stax.Dense(len(classes)),\n",
        "    stax.Softmax\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PUH1FeCZYqv"
      },
      "outputs": [],
      "source": [
        "# Abrir a planilha\n",
        "planilha = gc.open(\"EP 1 - NN vs CNN - Fashion Mnist usando JAX\")\n",
        "aba = planilha.worksheet(\"Página2\")\n",
        "\n",
        "n_coluna_DATA = 1 # O número da coluna contendo a data e a hora\n",
        "n_coluna_A = 2 # Dimensão do peso 1\n",
        "n_coluna_B = 3 # Dimensão do bibias 1\n",
        "n_coluna_C = 4 # Dimensão do peso 2\n",
        "n_coluna_D = 5 # Dimensão do bias 2\n",
        "n_coluna_E = 6 # Dimensão do peso 3\n",
        "n_coluna_F = 7 # Dimensão do bias 3\n",
        "n_coluna_G = 8 # Dimensão do peso 4\n",
        "n_coluna_H = 9 # Dimensão do bias 4\n",
        "n_coluna_I = 10 # Erro no treinamento\n",
        "n_coluna_J = 11 # Número de iterações\n",
        "n_coluna_K = 12 # Batch size\n",
        "n_coluna_L = 13 # Quantidade de amostras de teste\n",
        "n_coluna_M = 14 # Quantidade de amostra total\n",
        "n_coluna_N = 15 # Taxa de aprendizado\n",
        "n_coluna_O = 16 # Erro no teste\n",
        "n_coluna_P = 17 # Tempo no teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3517c0d0-a28f-41c6-a8a9-28b4a87aa4fd",
        "id": "YUImIFuqZb0Y"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights : (3, 3, 1, 32), Biases : (1, 1, 1, 32)\n",
            "Weights : (3, 3, 32, 16), Biases : (1, 1, 1, 16)\n",
            "Weights : (784, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = conv_init4(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86c7696-53fa-45c4-fd16-572fdc36861e",
        "id": "VlNRFITRZeQG"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[0.11346854, 0.11503609, 0.12250461, 0.08987208, 0.08465108,\n",
              "        0.06523983, 0.10706779, 0.09952015, 0.09868933, 0.10395041],\n",
              "       [0.12661624, 0.12114603, 0.10464215, 0.09760939, 0.07653171,\n",
              "        0.0618769 , 0.1185155 , 0.0857824 , 0.09442716, 0.11285254],\n",
              "       [0.11161763, 0.10252924, 0.10303943, 0.10422694, 0.0890063 ,\n",
              "        0.08569656, 0.09722918, 0.10129583, 0.10760707, 0.09775182],\n",
              "       [0.11328106, 0.1094126 , 0.10847753, 0.10078198, 0.07931047,\n",
              "        0.08214597, 0.10063422, 0.09821174, 0.10790451, 0.09983996],\n",
              "       [0.11225218, 0.1120536 , 0.10489712, 0.11213328, 0.08322569,\n",
              "        0.06219784, 0.09779106, 0.09799976, 0.12137157, 0.09607792]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "preds = conv_apply4(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31Tt_bX_ZqAC"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLossConv4(weights, input_data, actual):\n",
        "    preds = conv_apply4(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bxif7adZuH4"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "def TrainModelInBatches(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLossConv4)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((3, 3, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((1, 1, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((3,3)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((3, 3, 32, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str((1, 1, 1, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str((3,3)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, str((12544, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_H, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_I, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_J, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_K, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_L, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_M, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_N, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLossConv4)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_O, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_P, float(t1-t0))\n",
        "\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3bffc66-9f98-42c1-b199-cb78067a39b3",
        "id": "tDFnDzPMZxc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossEntropyLoss : 333.543\n",
            "CrossEntropyLoss : 192.880\n",
            "CrossEntropyLoss : 162.156\n",
            "CrossEntropyLoss : 144.185\n",
            "CrossEntropyLoss : 132.848\n",
            "CrossEntropyLoss : 124.941\n",
            "CrossEntropyLoss : 118.918\n",
            "CrossEntropyLoss : 114.187\n",
            "CrossEntropyLoss : 110.250\n",
            "CrossEntropyLoss : 106.891\n",
            "CrossEntropyLoss : 103.960\n",
            "CrossEntropyLoss : 101.355\n",
            "CrossEntropyLoss : 99.083\n",
            "CrossEntropyLoss : 97.043\n",
            "CrossEntropyLoss : 95.194\n",
            "CrossEntropyLoss : 93.542\n",
            "CrossEntropyLoss : 92.038\n",
            "CrossEntropyLoss : 90.652\n",
            "CrossEntropyLoss : 89.373\n",
            "CrossEntropyLoss : 88.196\n",
            "CrossEntropyLoss : 87.094\n",
            "CrossEntropyLoss : 86.051\n",
            "CrossEntropyLoss : 85.080\n",
            "CrossEntropyLoss : 84.161\n",
            "CrossEntropyLoss : 83.287\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 25\n",
        "batch_size=256\n",
        "\n",
        "weights = conv_init4(rng, (18,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXAoYsKVZ0zi"
      },
      "outputs": [],
      "source": [
        "def MakePredictions4(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        if X_batch.shape[0] != 0:\n",
        "            preds.append(conv_apply4(weights, X_batch))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc0a3b7-b513-4180-b90d-3064d857509f",
        "id": "Y-EaN-TcZ4AL"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([9, 2, 1, 1, 6], dtype=int32), Array([9, 0, 0, 3, 3], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "test_preds = MakePredictions4(opt_get_weights(final_opt_state), X_test, batch_size=batch_size)\n",
        "\n",
        "test_preds = jnp.concatenate(test_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "test_preds = jnp.argmax(test_preds, axis=1)\n",
        "\n",
        "train_preds = MakePredictions4(opt_get_weights(final_opt_state), X_train, batch_size=batch_size)\n",
        "\n",
        "train_preds = jnp.concatenate(train_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "train_preds = jnp.argmax(train_preds, axis=1)\n",
        "\n",
        "test_preds[:5], train_preds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c8c3ed9-8117-4c77-db7a-11b0e9a50cd6",
        "id": "hffyvsJ9Z616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.891\n",
            "Test  Accuracy : 0.880\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(Y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(Y_test, test_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04de85b-695f-469c-8827-54d7b3772e46",
        "id": "ylza_rW2Z-5p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Classification Report \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.85      0.83      1000\n",
            "         1.0       0.99      0.97      0.98      1000\n",
            "         2.0       0.80      0.83      0.82      1000\n",
            "         3.0       0.84      0.91      0.87      1000\n",
            "         4.0       0.81      0.77      0.79      1000\n",
            "         5.0       0.97      0.96      0.97      1000\n",
            "         6.0       0.72      0.64      0.68      1000\n",
            "         7.0       0.93      0.94      0.94      1000\n",
            "         8.0       0.96      0.97      0.97      1000\n",
            "         9.0       0.95      0.95      0.95      1000\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Test Classification Report \")\n",
        "print(classification_report(Y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ThkXYNPqZpsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN 5 - Conv+RELu, AvgPool, Conv+RELu, AvgPool, 784, 392 e 10"
      ],
      "metadata": {
        "id": "rgyrFR-WZjax"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbCED7M4zgVv"
      },
      "outputs": [],
      "source": [
        "conv_init5, conv_apply5 = stax.serial(\n",
        "    stax.Conv(32,(3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.AvgPool((3,3), padding=\"SAME\"),\n",
        "    stax.Conv(16, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.AvgPool((3,3), padding=\"SAME\"),\n",
        "\n",
        "    stax.Flatten,\n",
        "    stax.Dense(784),\n",
        "    stax.Relu,\n",
        "    stax.Dense(392),\n",
        "    stax.Relu,\n",
        "    stax.Dense(len(classes)),\n",
        "    stax.Softmax\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q-CkFvizk9C"
      },
      "outputs": [],
      "source": [
        "# Abrir a planilha\n",
        "planilha = gc.open(\"EP 1 - NN vs CNN - Fashion Mnist usando JAX\")\n",
        "aba = planilha.worksheet(\"Página2\")\n",
        "\n",
        "n_coluna_DATA = 1 # O número da coluna contendo a data e a hora\n",
        "n_coluna_A = 2 # Dimensão do peso 1\n",
        "n_coluna_B = 3 # Dimensão do bias 1\n",
        "n_coluna_C = 4 # Dimensão do peso 2\n",
        "n_coluna_D = 5 # Dimensão do bias 2\n",
        "n_coluna_E = 6 # Dimensão do peso 3\n",
        "n_coluna_F = 7 # Dimensão do bias 3\n",
        "n_coluna_G = 8 # Dimensão do peso 4\n",
        "n_coluna_H = 9 # Dimensão do bias 4\n",
        "n_coluna_I = 10 # Dimensão do peso 5\n",
        "n_coluna_J = 11 # Dimensão do bias 5\n",
        "n_coluna_K = 12 # Dimensão do peso 6\n",
        "n_coluna_L = 13 # Dimensão do bias 6\n",
        "n_coluna_M = 14 # Erro no treinamento\n",
        "n_coluna_N = 15 # Número de iterações\n",
        "n_coluna_O = 16 # Batch size\n",
        "n_coluna_P = 17 # Quantidade de amostras de teste\n",
        "n_coluna_Q = 18 # Quantidade de amostra total\n",
        "n_coluna_R = 19 # Taxa de aprendizado\n",
        "n_coluna_S = 20 # Erro no teste\n",
        "n_coluna_T = 21 # Tempo no teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548de604-8de5-491a-bc73-d0616ab5ca06",
        "id": "MolurWesztcV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights : (3, 3, 1, 32), Biases : (1, 1, 1, 32)\n",
            "Weights : (3, 3, 32, 16), Biases : (1, 1, 1, 16)\n",
            "Weights : (12544, 784), Biases : (784,)\n",
            "Weights : (784, 392), Biases : (392,)\n",
            "Weights : (392, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = conv_init5(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0e10e5-4f85-4987-8ab6-187d4c7af5ce",
        "id": "Z1xLU2raztce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[0.08712355, 0.1106542 , 0.11023826, 0.10060045, 0.10000064,\n",
              "        0.0971093 , 0.10290173, 0.09644327, 0.09649688, 0.09843171],\n",
              "       [0.08857621, 0.11876537, 0.1066256 , 0.10168226, 0.09600412,\n",
              "        0.10515672, 0.09139074, 0.09814203, 0.0953457 , 0.09831124],\n",
              "       [0.09616838, 0.10643027, 0.10425208, 0.10144627, 0.09744291,\n",
              "        0.10131925, 0.09687588, 0.09902634, 0.0976822 , 0.09935643],\n",
              "       [0.0911025 , 0.10938841, 0.10549676, 0.1026509 , 0.09709535,\n",
              "        0.10177306, 0.09533088, 0.10234383, 0.09711587, 0.09770246],\n",
              "       [0.08662689, 0.11696312, 0.10433838, 0.10052797, 0.09732302,\n",
              "        0.10304537, 0.09425217, 0.10079762, 0.09916727, 0.09695816]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "preds = conv_apply5(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGTKh6oKztce"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLossConv5(weights, input_data, actual):\n",
        "    preds = conv_apply5(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofxJCr53ztce"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "def TrainModelInBatches(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLossConv5)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((3, 3, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((1, 1, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((3,3)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((3, 3, 32, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str((1, 1, 1, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str((3,3)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, str((12544, 784)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_H, str((784,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_I, str((784, 392)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_J, str((392,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_K, str((392, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_L, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_M, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_N, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_O, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_P, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_Q, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_R, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLossConv5)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_S, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_T, float(t1-t0))\n",
        "\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1e3f69-a546-4196-8003-3517bced08e4",
        "id": "EbcXPwDiztce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossEntropyLoss : 228.992\n",
            "CrossEntropyLoss : 147.848\n",
            "CrossEntropyLoss : 130.127\n",
            "CrossEntropyLoss : 119.748\n",
            "CrossEntropyLoss : 112.216\n",
            "CrossEntropyLoss : 106.669\n",
            "CrossEntropyLoss : 102.270\n",
            "CrossEntropyLoss : 98.411\n",
            "CrossEntropyLoss : 95.277\n",
            "CrossEntropyLoss : 92.520\n",
            "CrossEntropyLoss : 90.084\n",
            "CrossEntropyLoss : 87.906\n",
            "CrossEntropyLoss : 85.822\n",
            "CrossEntropyLoss : 84.092\n",
            "CrossEntropyLoss : 82.436\n",
            "CrossEntropyLoss : 80.748\n",
            "CrossEntropyLoss : 79.194\n",
            "CrossEntropyLoss : 77.764\n",
            "CrossEntropyLoss : 76.440\n",
            "CrossEntropyLoss : 75.028\n",
            "CrossEntropyLoss : 73.847\n",
            "CrossEntropyLoss : 72.577\n",
            "CrossEntropyLoss : 71.449\n",
            "CrossEntropyLoss : 70.359\n",
            "CrossEntropyLoss : 69.285\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 25\n",
        "batch_size=256\n",
        "\n",
        "weights = conv_init5(rng, (18,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmlK623yztcf"
      },
      "outputs": [],
      "source": [
        "def MakePredictions5(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        if X_batch.shape[0] != 0:\n",
        "            preds.append(conv_apply5(weights, X_batch))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d95f135-f8bb-48a3-de65-a4f9fbecb435",
        "id": "48CDj7A6ztcf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([9, 2, 1, 1, 6], dtype=int32), Array([9, 0, 0, 3, 3], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "test_preds = MakePredictions5(opt_get_weights(final_opt_state), X_test, batch_size=batch_size)\n",
        "\n",
        "test_preds = jnp.concatenate(test_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "test_preds = jnp.argmax(test_preds, axis=1)\n",
        "\n",
        "train_preds = MakePredictions5(opt_get_weights(final_opt_state), X_train, batch_size=batch_size)\n",
        "\n",
        "train_preds = jnp.concatenate(train_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "train_preds = jnp.argmax(train_preds, axis=1)\n",
        "\n",
        "test_preds[:5], train_preds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e20e65c-1833-4937-e153-e18e895673ef",
        "id": "YVVe2nKjztcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.896\n",
            "Test  Accuracy : 0.875\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(Y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(Y_test, test_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98abb76b-2099-4c67-cf40-71fd44bbb217",
        "id": "uXBNu24cztcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Classification Report \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.86      0.83      1000\n",
            "         1.0       0.99      0.97      0.98      1000\n",
            "         2.0       0.71      0.85      0.77      1000\n",
            "         3.0       0.85      0.92      0.88      1000\n",
            "         4.0       0.78      0.81      0.79      1000\n",
            "         5.0       0.98      0.95      0.96      1000\n",
            "         6.0       0.83      0.52      0.64      1000\n",
            "         7.0       0.92      0.97      0.94      1000\n",
            "         8.0       0.96      0.98      0.97      1000\n",
            "         9.0       0.96      0.94      0.95      1000\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.87     10000\n",
            "weighted avg       0.88      0.88      0.87     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Test Classification Report \")\n",
        "print(classification_report(Y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkKGHFJzDK50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN 6 - Conv+RELu, MaxPool, Conv+RELu, MaxPool, 784, 392 e 10"
      ],
      "metadata": {
        "id": "vQLsFL8r2hNK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycDhOF4j2r1L"
      },
      "outputs": [],
      "source": [
        "conv_init6, conv_apply6 = stax.serial(\n",
        "    stax.Conv(32,(3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool(window_shape=(3,3), strides=(2,2), padding=\"SAME\"),\n",
        "    stax.Conv(16, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool(window_shape=(3,3), strides=(2,2), padding=\"SAME\"),\n",
        "\n",
        "    stax.Flatten,\n",
        "    stax.Dense(784),\n",
        "    stax.Relu,\n",
        "    stax.Dense(392),\n",
        "    stax.Relu,\n",
        "    stax.Dense(len(classes)),\n",
        "    stax.Softmax\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lBAfEkW2r1L"
      },
      "outputs": [],
      "source": [
        "# Abrir a planilha\n",
        "planilha = gc.open(\"EP 1 - NN vs CNN - Fashion Mnist usando JAX\")\n",
        "aba = planilha.worksheet(\"Página2\")\n",
        "\n",
        "n_coluna_DATA = 1 # O número da coluna contendo a data e a hora\n",
        "n_coluna_A = 2 # Dimensão do peso 1\n",
        "n_coluna_B = 3 # Dimensão do bibias 1\n",
        "n_coluna_C = 4 # Dimensão do peso 2\n",
        "n_coluna_D = 5 # Dimensão do bias 2\n",
        "n_coluna_E = 6 # Dimensão do peso 3\n",
        "n_coluna_F = 7 # Dimensão do bias 3\n",
        "n_coluna_G = 8 # Dimensão do peso 4\n",
        "n_coluna_H = 9 # Dimensão do bias 4\n",
        "n_coluna_I = 10 # Dimensão do peso 5\n",
        "n_coluna_J = 11 # Dimensão do bias 5\n",
        "n_coluna_K = 12 # Dimensão do peso 6\n",
        "n_coluna_L = 13 # Dimensão do bias 6\n",
        "n_coluna_M = 14 # Erro no treinamento\n",
        "n_coluna_N = 15 # Número de iterações\n",
        "n_coluna_O = 16 # Batch size\n",
        "n_coluna_P = 17 # Quantidade de amostras de teste\n",
        "n_coluna_Q = 18 # Quantidade de amostra total\n",
        "n_coluna_R = 19 # Taxa de aprendizado\n",
        "n_coluna_S = 20 # Erro no teste\n",
        "n_coluna_T = 21 # Tempo no teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ef9bb1-116d-4d99-db85-2407a1fd5a94",
        "id": "WMYKz26E2r1L"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights : (3, 3, 1, 32), Biases : (1, 1, 1, 32)\n",
            "Weights : (3, 3, 32, 16), Biases : (1, 1, 1, 16)\n",
            "Weights : (784, 784), Biases : (784,)\n",
            "Weights : (784, 392), Biases : (392,)\n",
            "Weights : (392, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = conv_init6(rng, (18,28,28,1))\n",
        "\n",
        "weights = weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40dbb901-e2d5-4c37-f597-6711f973a6ae",
        "id": "ljvbZ7Qc2r1M"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[0.09757035, 0.10241958, 0.0900976 , 0.11813903, 0.10072365,\n",
              "        0.082774  , 0.08404551, 0.10989817, 0.11372444, 0.10060761],\n",
              "       [0.10525773, 0.10380331, 0.08879522, 0.11272974, 0.09696601,\n",
              "        0.08687066, 0.08535547, 0.10144036, 0.1212389 , 0.0975426 ],\n",
              "       [0.10229913, 0.10589906, 0.0968143 , 0.10394232, 0.09907755,\n",
              "        0.09173166, 0.09412654, 0.09975762, 0.10959445, 0.09675736],\n",
              "       [0.10250545, 0.10784738, 0.09309515, 0.10935327, 0.09636433,\n",
              "        0.09162021, 0.09051629, 0.10029337, 0.111427  , 0.09697749],\n",
              "       [0.09967467, 0.10568917, 0.09336875, 0.11485872, 0.09853793,\n",
              "        0.08356208, 0.08581802, 0.0999238 , 0.12121885, 0.09734792]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "preds = conv_apply6(weights, X_train[:5])\n",
        "\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpVF9PZF2r1M"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLossConv6(weights, input_data, actual):\n",
        "    preds = conv_apply6(weights, input_data)\n",
        "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
        "    log_preds = jnp.log(preds)\n",
        "    return - jnp.sum(one_hot_actual * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcupbCyU2r1M"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "def TrainModelInBatches(X, Y, epochs, opt_state, batch_size=32):\n",
        "    for i in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
        "        agora = datetime.now(fuso_horario) # Atualiza o horário\n",
        "        proxima_linha_vazia = len(aba.get_all_values()) + 1\n",
        "        aba.update_cell(proxima_linha_vazia, # Aqui vai a linha que vai ser adicionada\n",
        "                  n_coluna_DATA,  # Aqui vai o número da coluna\n",
        "                  agora.strftime(\"%d/%m/%Y %H:%M:%S\")) # Add a coluna de informações a data e hora\n",
        "\n",
        "        losses = [] ## Record loss of each batch\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "            else:\n",
        "                start, end = int(batch*batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLossConv6)(opt_get_weights(opt_state), X_batch,Y_batch)\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss) ## Record Loss\n",
        "\n",
        "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
        "        t1=time.time()\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_A, str((3, 3, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_B, str((1, 1, 1, 32)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_C, str((3,3)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_D, str((3, 3, 32, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_E, str((1, 1, 1, 16)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_F, str((3,3)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_G, str((12544, 784)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_H, str((784,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_I, str((784, 392)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_J, str((392,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_K, str((392, 10)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_L, str((10,)))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_M, float(jnp.array(losses).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_N, str(i))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_O, str(batch_size))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_P, str(X_train.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_Q, str(X_test.shape))\n",
        "        aba.update_cell(proxima_linha_vazia,n_coluna_R, str(learning_rate))\n",
        "        TestError, TestGrad = value_and_grad(CrossEntropyLossConv6)(opt_get_weights(opt_state), X_test,Y_test)\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_S, float(jnp.array(TestError).mean()))\n",
        "        aba.update_cell(proxima_linha_vazia, n_coluna_T, float(t1-t0))\n",
        "\n",
        "\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d59f4171-4d95-4038-e5dc-8add4ea1bc2f",
        "id": "NCaz0u672r1M"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossEntropyLoss : 279.547\n",
            "CrossEntropyLoss : 166.982\n",
            "CrossEntropyLoss : 141.139\n",
            "CrossEntropyLoss : 126.828\n",
            "CrossEntropyLoss : 117.492\n",
            "CrossEntropyLoss : 110.436\n",
            "CrossEntropyLoss : 104.791\n",
            "CrossEntropyLoss : 100.109\n",
            "CrossEntropyLoss : 96.048\n",
            "CrossEntropyLoss : 92.522\n",
            "CrossEntropyLoss : 89.380\n",
            "CrossEntropyLoss : 86.645\n",
            "CrossEntropyLoss : 84.113\n",
            "CrossEntropyLoss : 81.884\n",
            "CrossEntropyLoss : 79.858\n",
            "CrossEntropyLoss : 77.857\n",
            "CrossEntropyLoss : 76.222\n",
            "CrossEntropyLoss : 74.567\n",
            "CrossEntropyLoss : 73.086\n",
            "CrossEntropyLoss : 71.738\n",
            "CrossEntropyLoss : 70.375\n",
            "CrossEntropyLoss : 69.084\n",
            "CrossEntropyLoss : 67.882\n",
            "CrossEntropyLoss : 66.781\n",
            "CrossEntropyLoss : 65.687\n"
          ]
        }
      ],
      "source": [
        "seed = jax.random.PRNGKey(123)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 25\n",
        "batch_size=256\n",
        "\n",
        "weights = conv_init6(rng, (18,28,28,1))\n",
        "weights = weights[1]\n",
        "\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.sgd(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "\n",
        "final_opt_state = TrainModelInBatches(X_train, Y_train, epochs, opt_state, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRrAmfEE2r1N"
      },
      "outputs": [],
      "source": [
        "def MakePredictions6(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        if X_batch.shape[0] != 0:\n",
        "            preds.append(conv_apply6(weights, X_batch))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a91c3294-252f-4bab-a6b5-3bf78ebc0c60",
        "id": "bawzmYrf2r1N"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([9, 2, 1, 1, 6], dtype=int32), Array([9, 0, 0, 3, 1], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "test_preds = MakePredictions6(opt_get_weights(final_opt_state), X_test, batch_size=batch_size)\n",
        "\n",
        "test_preds = jnp.concatenate(test_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "test_preds = jnp.argmax(test_preds, axis=1)\n",
        "\n",
        "train_preds = MakePredictions6(opt_get_weights(final_opt_state), X_train, batch_size=batch_size)\n",
        "\n",
        "train_preds = jnp.concatenate(train_preds).squeeze() ## Combine predictions of all batches\n",
        "\n",
        "train_preds = jnp.argmax(train_preds, axis=1)\n",
        "\n",
        "test_preds[:5], train_preds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bed285f-6581-40e3-8d20-ae5bab5df55d",
        "id": "U964qApZ2r1N"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.910\n",
            "Test  Accuracy : 0.893\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Train Accuracy : {:.3f}\".format(accuracy_score(Y_train, train_preds)))\n",
        "print(\"Test  Accuracy : {:.3f}\".format(accuracy_score(Y_test, test_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee34880-947d-4817-e1ef-f180bba85f67",
        "id": "8UiwQXQ22r1N"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Classification Report \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.91      0.84      1000\n",
            "         1.0       0.99      0.97      0.98      1000\n",
            "         2.0       0.80      0.86      0.83      1000\n",
            "         3.0       0.87      0.93      0.90      1000\n",
            "         4.0       0.83      0.81      0.82      1000\n",
            "         5.0       0.98      0.97      0.97      1000\n",
            "         6.0       0.81      0.59      0.68      1000\n",
            "         7.0       0.94      0.96      0.95      1000\n",
            "         8.0       0.97      0.98      0.98      1000\n",
            "         9.0       0.96      0.95      0.96      1000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Test Classification Report \")\n",
        "print(classification_report(Y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a28BtAtIVALK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "O9VBKErM8ppW",
        "b8JJ_mHaMnOA"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}